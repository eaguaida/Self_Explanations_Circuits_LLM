# Looking for Circuits responsible for Self-Explanations in Large Language Models

## Introduction
My idea is to find circuits that are activated after the LLM is asked to give an "explanation." One method was concatenating the previous prompt and answer into the following prompt. However, I thought it might just generate an explanation based on the previous context, so I attempted to include everything in the same prompt.

I specified three prompts in a similar context but with different subjects, wrote six explanations myself, and appended three self-explanations generated by the model. Initially, the explanations I wrote were too different, making it hard for the transformers to produce similar text without seeing the whole context.

So, I slightly cheated in this part by setting the temperature to 0.10 and writing somewhat similar explanations to make them as alike as possible. What I was actually interested in was seeing how close these self-explanations could be to a "good explanation."

Applying the logits difference with "Self-Explanation" - "Explanation Provided" still gave me some high numbers, and I think I didn't fully understand what I was doing at some points. However, sometimes I could see explanations going in the opposite direction, like "Henry didn't run away because humans are not scared of wolves," when in fact, humans are supposed to be scared of wolves. Also, the outcomes seem to vary a lot depending on whether I ask "Why?" or "Can you explain?"

## My opinion
I think that after applying Causal Scrubbing, some interesting results could come up, and by analyzing the circuits, we might understand what happens when a model enters "self-explaining mode." So, what if we could actually find an algorithm that performs this "explanation" in the background? This algorithm could provide explanations that contribute to future actions.

After some research, I think many scientists have been questioning whether we can trust these "self-explanations," as it's quite common for an LLM to give a wrong answer and then confidently explain it!

My idea is that if the model could question itself (perhaps creating a Q_Explanation Matrix), this matrix could store meaningful information from each prior answer, discard the useless information, and reduce the context window in some way.

## Future research
This idea might not sound that interesting for a chat-based LLM, but if we expand an LLM to act as a Broca's area to AI agents, we could create agents that store explanations for every decision/conversation. This "questioning" of its own decision could be used to improve their behaviors.

In human terms, this could be like reducing impulsive behaviors. For example, if a random person shouts at you on the street, the probability that you will shout back is high, but most of the time the rational action is to just ignore them and continue your way.

We question our future decisions every time. If we want agents aligned to humans, we want them to question their decisions as well.

Anyway, this is just an initial idea! But I think that something that differentiates us from other species is our ability to explain most of the decisions we plan to make and to question ourselves about them.
